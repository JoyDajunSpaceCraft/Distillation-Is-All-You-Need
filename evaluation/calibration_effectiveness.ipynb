{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2c21925-f31f-4710-9140-18d71df1e847",
   "metadata": {},
   "source": [
    "## Evaluate Calibration effectiveness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "853394ab-f184-4632-ba6b-a0cec0dcbb95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kendall's Tau Coefficient: 0.652941176470588\n",
      "P-value: 0.31764705882352934\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "def clean_reason(permutation):\n",
    "  # 找到第一次出现的排名序列\n",
    "  ranking_pattern = re.findall(r'\\[\\d+\\] >', permutation)\n",
    "  new_response = \"\"\n",
    "  # 提取这些模式中的数字\n",
    "  ranking_numbers = [int(num) for pattern in ranking_pattern for num in re.findall(r'\\d+', pattern)]\n",
    "  new_response = \" \".join([str(i )for i in ranking_numbers])\n",
    "  # 单独处理最后一个排名元素\n",
    "  # 查找文本中的最后一个 [数字] 模式\n",
    "  last_number_match = re.search(r'\\[\\d+\\](?!.*\\[\\d+\\])', permutation)\n",
    "  if last_number_match:\n",
    "      last_number = last_number_match.group().strip('[]')\n",
    "      new_response +=  \" \" + last_number\n",
    "      # ranking_numbers.append(str(last_number))\n",
    "  return new_response\n",
    "T5_path = \"../data/T5_results/msmarco_test_t5.jsonl\"\n",
    "GPT2_path = \"../data/GPT2_results/msmarco_test_gpt2.jsonl\"\n",
    "t5_ranks = []\n",
    "gpt35_ranks = [] \n",
    "with open(T5_path, \"r\") as f:\n",
    "    data = []\n",
    "    for i in f.readlines():\n",
    "        item = json.loads(i)\n",
    "        data.append(item)\n",
    "        \n",
    "        res = clean_reason(item[\"gpt2_reason\"])\n",
    "        res = res.split()\n",
    "        res = [int(i) for i in res]\n",
    "        res = list(set(res))\n",
    "        for num in range(1, 6):\n",
    "            if num not in res:\n",
    "                res.append(num)\n",
    "        count_larger = 0\n",
    "        for num in res:\n",
    "            if num >5:\n",
    "                res.remove(num)\n",
    "                count_larger+=1\n",
    "        if count_larger>=1:\n",
    "            continue\n",
    "        gptres = [int(i) for i in item[\"re_rank_id\"]]\n",
    "        gptres = list(set(gptres))\n",
    "        for num in range(1, 6):\n",
    "            if num not in gptres:\n",
    "                gptres.append(num)\n",
    "        gpt35_ranks.append(gptres)\n",
    "        # print(res)\n",
    "        t5_ranks.append(res)\n",
    "\n",
    "# Kendall's Tau\n",
    "from scipy.stats import kendalltau\n",
    "\n",
    "taus = []\n",
    "p_values = []\n",
    "for gpt35, t5 in zip(gpt35_ranks,t5_ranks):\n",
    "    # gpt_ranks = [1, 2, 4, 3, 5]  # GPT的排名\n",
    "    # t5_ranks = [3, 1, 5, 4, 2]  # T5的排名\n",
    "    \n",
    "    # 计算Kendall's Tau系数\n",
    "    tau, p_value = kendalltau(gpt35, t5)\n",
    "    taus.append(tau)\n",
    "    p_values.append(p_value)\n",
    "print(\"Kendall's Tau Coefficient:\", sum(taus)/len(taus))\n",
    "print(\"P-value:\", sum(p_values)/len(p_values))\n",
    "# GPT2\n",
    "# Kendall's Tau Coefficient: 0.652941176470588\n",
    "# P-value: 0.31764705882352934\n",
    "# T5 \n",
    "# Kendall's Tau Coefficient: 0.802\n",
    "# P-value: 0.15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5418aa6b-8a3c-4ef5-a0b6-4b84ea1084f8",
   "metadata": {},
   "source": [
    "## Evaluate similiarity between different reasons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "81014388-89b7-4b9a-a92c-3f70c598ade3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.246220399521403, 0.3769048847428245, 0.3481947502162826, 0.42543153250196997, 0.309233521418506, 0.2567397198976442, 0.2896656907634423, 0.2143500703590344, 0.3599341219059331, 0.35396645544195604, 0.2391733136396949, 0.24934297753445098, 0.30986212877568897, 0.20640245834866996, 0.2610240840814875, 0.1311377927328061, 0.36652489391592347, 0.3503583678833506, 0.3498927134375653, 0.2537443117512478, 0.27204758534413254, 0.36452848372868474, 0.24748125052269612, 0.34916042893236166, 0.28430560410243333, 0.25089044806864697, 0.37697756402323734, 0.3599045632904195, 0.26895074881080494, 0.33032869033150486, 0.18209529504901367, 0.4123686409037216, 0.23900270969558424, 0.44192292898227353, 0.41056051463836896, 0.3516133363074111, 0.47209832505949845, 0.25493537297882823, 0.2787231503699099, 0.2554791976832552, 0.3972712774371111, 0.28028283644511964, 0.30150706595716675, 0.26186615316185013, 0.2774183775063283, 0.30191148839391146, 0.4094107136879803, 0.20497882756027203, 0.3234050904768624, 0.26691003128142643, 0.23927846582512582, 0.21327794406048553, 0.19481934796968123, 0.321008664806154, 0.17473684051242291, 0.35313593469357524]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "T5_path = \"../data/T5_results/msmarco_test_t5.jsonl\"\n",
    "gpt3_texts = []\n",
    "t5_texts = []\n",
    "with open(T5_path, \"r\") as f:\n",
    "    data = []\n",
    "    for i in f.readlines():\n",
    "        item = json.loads(i)\n",
    "        data.append(item)\n",
    "        t5_texts.append(item[\"t5_reason\"])\n",
    "        gpt3_texts.append(item[\"reason\"])\n",
    "        \n",
    "# GPT 和 T5 的原始解释文本\n",
    "# gpt_text = \"\"\"reason\"[1] > [2] > [4] > [3] > [5] I ranked passage [1] as the most relevant because it directly provides a definition of declaratory judgment, explaining that it declares the rights, duties, or obligations of each party in a dispute without ordering any action or awarding damages. Passage [2] is ranked next as it also defines declaratory judgment by stating that it resolves a dispute by stating a fact, such as ownership of property or patents. Passage [4] is ranked third as it further elaborates on the types of situations where declaratory judgments are sought, such as determining rights under specific laws or regulations. Passage [3] is ranked fourth as it discusses a factor that supports the dismissal of a declaratory judgment action, which is relevant but not as directly related to defining declaratory judgment as the previous passages. Passage [5] is ranked last as it provides options available to insurers in certain situations, which is relevant but not as directly related to defining declaratory judgment as the other passages.\"\"\"\n",
    "# t5_text = \"\"\"Here is the ranking of the passages based on their relevance to the search query \"definition declaratory judgment\": [3] > [1] > [5] > [4] > [2] Reasoning: - [3] provides a clear definition of declaratory judgment, providing a clear definition of the term. - [1] discusses the definition of declaratory judgment, which is relevant to understanding the concept. - [5] discusses the role of insurers in defending the insured, which is relevant to understanding the concept. - [4] discusses the conduct of insurers in filing declaratory judgment actions, which is relevant to understanding the concept. - [2] discusses the definition of declaratory judgment, which is relevant to understanding the concept.\"\"\"\n",
    "\n",
    "# def extract_explanations_gpt3(text):\n",
    "#     \"\"\" Extract explanations from the provided GPT text based on passage identifiers. \"\"\"\n",
    "#     explanations = {}\n",
    "#     matches = re.finditer(r\"Passage \\[(\\d+)\\] (is ranked .*?)(?= Passage|$)\", text, re.DOTALL)\n",
    "#     for match in matches:\n",
    "#         index = int(match.group(1))\n",
    "#         explanation = match.group(2).strip()\n",
    "#         explanations[index] = explanation\n",
    "#     return explanations\n",
    "def extract_explanations_gpt3(text):\n",
    "    \"\"\" Extract explanations from the provided GPT text based on passage identifiers. \"\"\"\n",
    "    explanations = {}\n",
    "    # 提取段落编号和对应的解释，处理多行解释的情况\n",
    "    matches = re.finditer(r\"\\[(\\d+)\\]:\\s*([^[]+)(?=\\[\\d+\\]:|$)\", text, re.DOTALL)\n",
    "    for match in matches:\n",
    "        index = int(match.group(1))\n",
    "        explanation = \" \".join(match.group(2).strip().split('\\n'))  # 去除多余的换行符，并将解释整理为单行\n",
    "        explanations[index] = explanation\n",
    "    return explanations\n",
    "\n",
    "def extract_explanations_t5(text):\n",
    "    \"\"\" Extract explanations from the provided T5 text based on passage identifiers. \"\"\"\n",
    "    explanations = {}\n",
    "    matches = re.finditer(r\"\\-\\s+\\[(\\d+)\\]\\s+(.*?)(?=\\s*\\- \\[|\\s*$)\", text, re.DOTALL)\n",
    "    for match in matches:\n",
    "        index = int(match.group(1))\n",
    "        explanation = match.group(2).strip()\n",
    "        explanations[index] = explanation\n",
    "    return explanations\n",
    "\n",
    "overall_sim = []\n",
    "work_example = 0\n",
    "for gpt3_text, t5_text in zip(gpt3_texts, t5_texts):\n",
    "    # 提取解释\n",
    "    gpt3_explanations = extract_explanations_gpt3(gpt3_text)\n",
    "    t5_explanations = extract_explanations_t5(t5_text)\n",
    "    # print(gpt3_explanations)\n",
    "    # 计算余弦相似度\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    all_texts = list(gpt3_explanations.values()) + list(t5_explanations.values())\n",
    "    tfidf_matrix = vectorizer.fit_transform(all_texts)\n",
    "    \n",
    "    # 分配索引\n",
    "    gpt3_indices = {index: i for i, index in enumerate(gpt3_explanations.keys())}\n",
    "    t5_indices = {index: i + len(gpt3_explanations) for i, index in enumerate(t5_explanations.keys())}\n",
    "    # print(gpt3_indices)\n",
    "    sum_sim = []\n",
    "    # 打印相似度结果\n",
    "    for index in gpt3_indices:\n",
    "        if index in t5_indices:\n",
    "            gpt_idx = gpt3_indices[index]\n",
    "            t5_idx = t5_indices[index]\n",
    "            similarity = cosine_similarity(tfidf_matrix[gpt_idx:gpt_idx+1], tfidf_matrix[t5_idx:t5_idx+1])[0][0]\n",
    "            sum_sim.append(similarity)\n",
    "            # print(f\"Similarity between GPT and T5 explanations for passage [{index}]: {similarity:.3f}\")\n",
    "    if len(sum_sim)==0:\n",
    "        continue\n",
    "    work_example+=1\n",
    "    overall_sim.append(sum(sum_sim)/len(sum_sim))\n",
    "print(overall_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8dd5ab04-3fb6-47e6-9276-3b8e756c3630",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.30040532299050265"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(overall_sim)/len(overall_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9444d428-dcd9-408f-b186-5b8eef6c9245",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 51\u001b[0m\n\u001b[1;32m     49\u001b[0m vectorizer \u001b[38;5;241m=\u001b[39m TfidfVectorizer()\n\u001b[1;32m     50\u001b[0m all_texts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(gpt3_explanations\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(gpt2_explanations\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[0;32m---> 51\u001b[0m tfidf_matrix \u001b[38;5;241m=\u001b[39m \u001b[43mvectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_texts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# 分配索引\u001b[39;00m\n\u001b[1;32m     54\u001b[0m gpt3_indices \u001b[38;5;241m=\u001b[39m {index: i \u001b[38;5;28;01mfor\u001b[39;00m i, index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(gpt3_explanations\u001b[38;5;241m.\u001b[39mkeys())}\n",
      "File \u001b[0;32m~/anaconda3/envs/llama_factory/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:1850\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1832\u001b[0m \u001b[38;5;124;03m\"\"\"Learn vocabulary and idf, return document-term matrix.\u001b[39;00m\n\u001b[1;32m   1833\u001b[0m \n\u001b[1;32m   1834\u001b[0m \u001b[38;5;124;03mThis is equivalent to fit followed by transform, but more efficiently\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1847\u001b[0m \u001b[38;5;124;03m    Tf-idf-weighted document-term matrix.\u001b[39;00m\n\u001b[1;32m   1848\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params()\n\u001b[0;32m-> 1850\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1851\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[1;32m   1852\u001b[0m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[1;32m   1853\u001b[0m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/llama_factory/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:1203\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1200\u001b[0m min_df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_df\n\u001b[1;32m   1201\u001b[0m max_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_features\n\u001b[0;32m-> 1203\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1204\u001b[0m \u001b[43m                                  \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1206\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[1;32m   1207\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/llama_factory/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:1134\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1132\u001b[0m     vocabulary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(vocabulary)\n\u001b[1;32m   1133\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vocabulary:\n\u001b[0;32m-> 1134\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mempty vocabulary; perhaps the documents only\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1135\u001b[0m                          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m contain stop words\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indptr[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39miinfo(np\u001b[38;5;241m.\u001b[39mint32)\u001b[38;5;241m.\u001b[39mmax:  \u001b[38;5;66;03m# = 2**31 - 1\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _IS_32BIT:\n",
      "\u001b[0;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "gpt2_path = \"../data/GPT2_results/msmarco_test_gpt2.jsonl\"\n",
    "gpt3_texts = []\n",
    "gpt2_texts = []\n",
    "with open(gpt2_path, \"r\") as f:\n",
    "    data = []\n",
    "    for i in f.readlines():\n",
    "        item = json.loads(i)\n",
    "        data.append(item)\n",
    "        gpt2_texts.append(item[\"gpt2_reason\"])\n",
    "        gpt3_texts.append(item[\"reason\"])\n",
    "        \n",
    "# GPT 和 T5 的原始解释文本\n",
    "# gpt_text = \"\"\"reason\"[1] > [2] > [4] > [3] > [5] I ranked passage [1] as the most relevant because it directly provides a definition of declaratory judgment, explaining that it declares the rights, duties, or obligations of each party in a dispute without ordering any action or awarding damages. Passage [2] is ranked next as it also defines declaratory judgment by stating that it resolves a dispute by stating a fact, such as ownership of property or patents. Passage [4] is ranked third as it further elaborates on the types of situations where declaratory judgments are sought, such as determining rights under specific laws or regulations. Passage [3] is ranked fourth as it discusses a factor that supports the dismissal of a declaratory judgment action, which is relevant but not as directly related to defining declaratory judgment as the previous passages. Passage [5] is ranked last as it provides options available to insurers in certain situations, which is relevant but not as directly related to defining declaratory judgment as the other passages.\"\"\"\n",
    "# t5_text = \"\"\"Here is the ranking of the passages based on their relevance to the search query \"definition declaratory judgment\": [3] > [1] > [5] > [4] > [2] Reasoning: - [3] provides a clear definition of declaratory judgment, providing a clear definition of the term. - [1] discusses the definition of declaratory judgment, which is relevant to understanding the concept. - [5] discusses the role of insurers in defending the insured, which is relevant to understanding the concept. - [4] discusses the conduct of insurers in filing declaratory judgment actions, which is relevant to understanding the concept. - [2] discusses the definition of declaratory judgment, which is relevant to understanding the concept.\"\"\"\n",
    "\n",
    "def extract_explanations_gpt3(text):\n",
    "    \"\"\" Extract explanations from the provided GPT text based on passage identifiers. \"\"\"\n",
    "    explanations = {}\n",
    "    matches = re.finditer(r\"Passage \\[(\\d+)\\] (is ranked .*?)(?= Passage|$)\", text, re.DOTALL)\n",
    "    for match in matches:\n",
    "        index = int(match.group(1))\n",
    "        explanation = match.group(2).strip()\n",
    "        explanations[index] = explanation\n",
    "    return explanations\n",
    "\n",
    "def extract_explanations_gpt2(text):\n",
    "    \"\"\" Extract explanations from the provided T5 text based on passage identifiers. \"\"\"\n",
    "    explanations = {}\n",
    "    matches = re.finditer(r\"\\-\\s+\\[(\\d+)\\]\\s+(.*?)(?=\\s*\\- \\[|\\s*$)\", text, re.DOTALL)\n",
    "    for match in matches:\n",
    "        index = int(match.group(1))\n",
    "        explanation = match.group(2).strip()\n",
    "        explanations[index] = explanation\n",
    "    return explanations\n",
    "\n",
    "overall_sim = []\n",
    "work_example = 0\n",
    "for gpt3_text, gpt2_text in zip(gpt3_texts, gpt2_texts):\n",
    "    # 提取解释\n",
    "    gpt3_explanations = extract_explanations_gpt3(gpt3_text)\n",
    "    gpt2_explanations = extract_explanations_gpt2(gpt2_text)\n",
    "\n",
    "    # print(\"t5_explanations\", t5_explanations)\n",
    "    # 计算余弦相似度\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    all_texts = list(gpt3_explanations.values()) + list(gpt2_explanations.values())\n",
    "    tfidf_matrix = vectorizer.fit_transform(all_texts)\n",
    "    \n",
    "    # 分配索引\n",
    "    gpt3_indices = {index: i for i, index in enumerate(gpt3_explanations.keys())}\n",
    "    gpt2_indices = {index: i + len(gpt3_explanations) for i, index in enumerate(gpt2_explanations.keys())}\n",
    "\n",
    "    sum_sim = []\n",
    "    # 打印相似度结果\n",
    "    for index in gpt3_indices:\n",
    "        if index in gpt2_indices:\n",
    "            gpt_idx = gpt3_indices[index]\n",
    "            gpt2_idx = gpt2_indices[index]\n",
    "            similarity = cosine_similarity(tfidf_matrix[gpt_idx:gpt_idx+1], tfidf_matrix[gpt2_idx:gpt2_idx+1])[0][0]\n",
    "            sum_sim.append(similarity)\n",
    "            # print(f\"Similarity between GPT and T5 explanations for passage [{index}]: {similarity:.3f}\")\n",
    "    if len(sum_sim)==0:\n",
    "        continue\n",
    "    work_example+=1\n",
    "    overall_sim.append(sum(sum_sim)/len(sum_sim))\n",
    "print(overall_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b39e196e-b650-4c93-a6ed-5c0c55251391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6767335681996629\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "t5_path = \"../data/T5_results/msmarco_test_t5.jsonl\"\n",
    "gpt3_texts = []\n",
    "t5_texts = []\n",
    "with open(t5_path, \"r\") as f:\n",
    "    data = []\n",
    "    for i in f.readlines():\n",
    "        item = json.loads(i)\n",
    "        data.append(item)\n",
    "        t5_texts.append(item[\"t5_reason\"])\n",
    "        gpt3_texts.append(item[\"reason\"])\n",
    "similarities = []        \n",
    "for t5_text, gpt3_text in zip(t5_texts, gpt3_texts):\n",
    "\n",
    "    # 计算余弦相似度\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    texts = [gpt3_text, t5_text]\n",
    "    tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "    \n",
    "    similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]\n",
    "    # print(f\"Overall Cosine Similarity between the two reason texts: {similarity:.3f}\")\n",
    "    similarities.append(similarity)\n",
    "print(sum(similarities)/len(similarities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "312e3a1b-f9e2-4676-9499-9c543bd2df71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34289941380256433\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "gpt2_path = \"../data/GPT2_results/msmarco_test_gpt2.jsonl\"\n",
    "gpt3_texts = []\n",
    "gpt2_texts = []\n",
    "with open(gpt2_path, \"r\") as f:\n",
    "    data = []\n",
    "    for i in f.readlines():\n",
    "        item = json.loads(i)\n",
    "        data.append(item)\n",
    "        gpt2_texts.append(item[\"gpt2_reason\"])\n",
    "        gpt3_texts.append(item[\"reason\"])\n",
    "similarities = []        \n",
    "for gpt2_text, gpt3_text in zip(gpt2_texts, gpt3_texts):\n",
    "\n",
    "    # 计算余弦相似度\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    texts = [gpt3_text, gpt2_text]\n",
    "    tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "    \n",
    "    similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]\n",
    "    # print(f\"Overall Cosine Similarity between the two reason texts: {similarity:.3f}\")\n",
    "    similarities.append(similarity)\n",
    "print(sum(similarities)/len(similarities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68daf5f4-2e7a-4c1c-b0d4-939284fb1e3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
