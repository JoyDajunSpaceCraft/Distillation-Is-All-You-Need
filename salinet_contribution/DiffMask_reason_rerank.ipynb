{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8e54de3-7840-4a2f-92dd-081a5eaa0c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuj49/anaconda3/envs/llama_factory/lib/python3.8/site-packages/requests/__init__.py:87: RequestsDependencyWarning: urllib3 (2.2.1) or chardet (4.0.0) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import lightning.pytorch as pl\n",
    "from functools import partial\n",
    "from jaxtyping import Float\n",
    "from diffmask import DiffMask\n",
    "from util.distributions import BinaryConcrete, RectifiedStreched\n",
    "from configuration.diffmask import DiffMaskConfig\n",
    "import json\n",
    "from torch.utils.data import Dataset\n",
    "from accelerate import Accelerator\n",
    "from transformers import AutoModelForSequenceClassification, AutoConfig, AutoTokenizer, AdamW\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from rank_loss import RankLoss\n",
    "import numpy as np\n",
    "import os\n",
    "import argparse\n",
    "import tempfile\n",
    "import copy\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81e1dd3-3664-49f4-b449-d6e7762429ea",
   "metadata": {},
   "source": [
    "## Data process "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1a99e3f-0d98-4821-8651-cea2c14cf3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sperate_reason(reason_text):\n",
    "  pattern = r'\\-\\s(?:(?:Passage\\s)?\\[(\\d+)\\](?:\\s?and\\s?(?:Passage\\s)?\\[(\\d+)\\])?)(.*?)(?=\\n\\-|\\n\\n|\\Z)'\n",
    "\n",
    "  # print(\"reason_text in sperate_reason\", reason_text)\n",
    "  matches = re.findall(pattern, reason_text, re.DOTALL)\n",
    "\n",
    "  # 初始化一个字典来存储每个文档的reason\n",
    "  reasons_dict = {}\n",
    "  if len(matches) < 5:\n",
    "    # 处理匹配结果\n",
    "    for match in matches:\n",
    "        # 提取文档编号和reason描述\n",
    "        doc_ids = match[:-1]  # 文档编号部分\n",
    "        doc_reason = match[-1].strip()  # reason描述部分\n",
    "\n",
    "        # 对每个文档编号进行处理\n",
    "        for doc_id in doc_ids:\n",
    "            if doc_id:  # 确保doc_id不为空\n",
    "                # 为每个文档编号存储或更新reason描述\n",
    "                if doc_id in reasons_dict:\n",
    "                    # 如果同一个文档编号对应多个reason，可以选择合并或选择性保留\n",
    "                    reasons_dict[doc_id] += \" \" + doc_reason\n",
    "                else:\n",
    "                    reasons_dict[doc_id] = doc_reason\n",
    "  else:\n",
    "    for match in matches:\n",
    "      # print(\"match\", match)\n",
    "      doc_id, _, doc_reason = match\n",
    "      reasons_dict[doc_id] = doc_reason\n",
    "          # print(f\"Document ID: {doc_id}, Reason: {doc_reason}\\n\")\n",
    "\n",
    "  if len(reasons_dict.keys()) <5:\n",
    "    \n",
    "    return None\n",
    "  \n",
    "\n",
    "  return reasons_dict\n",
    "\n",
    "\n",
    "def receive_response(data, reason_name=\"reason\"):\n",
    "    \n",
    "    responses = [item[\"re_rank_id\"] for item in data]\n",
    "    \n",
    "    def remove_duplicate(response):\n",
    "        new_response = []\n",
    "        for c in response:\n",
    "            if c not in new_response:\n",
    "                new_response.append(c)\n",
    "        return new_response\n",
    "\n",
    "    new_data = []\n",
    "    unsorted_score = []\n",
    "    for item, response in zip(data, responses):\n",
    "        \n",
    "        reasons = item[reason_name]\n",
    "        reasons_dict = sperate_reason(reasons)\n",
    "        \n",
    "        passages = item['unsorted_docs']\n",
    "        \n",
    "        unsorted_reasoned_response = [] \n",
    "        \n",
    "        if reasons_dict!=None:\n",
    "            for idx, passage in enumerate(passages):\n",
    "                unsorted_reasoned_response.append(passage + \"reason\" +reasons_dict[str(idx+1)] )\n",
    "        else:\n",
    "            unsorted_reasoned_response = [\"\"]*5\n",
    "        \n",
    "        \n",
    "        response = [int(x) - 1 for x in response]\n",
    "        response = remove_duplicate(response)\n",
    "        \n",
    "        original_rank = [tt for tt in range(len(passages))]\n",
    "        response = [ss for ss in response if ss in original_rank]\n",
    "        response = response + [tt for tt in original_rank if tt not in response]\n",
    "        \n",
    "        new_passages = [passages[ii] for ii in response]\n",
    "        new_reason_passage = [unsorted_reasoned_response[ii] for ii in response]\n",
    "        unsorted_score = item[\"scores\"]\n",
    "\n",
    "        \n",
    "        new_data.append({'query': item['query'],\n",
    "                         'retrieved_passages': new_passages,\n",
    "                         'reasoned_passages':new_reason_passage,\n",
    "                        'unsorted_score':unsorted_score})\n",
    "    return new_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a757062-6ca4-4f86-a959-d4164291951c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_path =\"../data/T5_results/msmarco_test_t5.jsonl\"\n",
    "data = [json.loads(line) for line in open(data_path)]\n",
    "data = receive_response(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dee42442-f83f-49a9-9b6b-592fa8bf62e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data [{\"query\":'', \"retrieved_passages\":[docs], \"reasoned_passages\":[],\"unsorted_score\":[]}]\n",
    "# print(len(data[0][\"reasoned_passages\"])) # 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a085d8c2-a870-4bcd-b624-4f99a937a0ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(docs_reasons) in prepare data <class 'list'>\n",
      "type(docs_reasons) in prepare data after <class 'generator'>\n",
      "train_data <class 'list'>\n",
      "171\n",
      "1\n",
      "type(docs_reasons) <class 'generator'>\n",
      "type(unsorted_scores) <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from os import path, makedirs\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class Dataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        query, docs, docs_reasons, unsorted_scores= self.data[index]\n",
    "        print(\"type(docs_reasons)\", type(docs_reasons))\n",
    "        print(\"type(unsorted_scores)\", type(unsorted_scores))\n",
    "        # return query, docs, docs_reasons, unsorted_scores\n",
    "        return query, docs, list(docs_reasons), unsorted_scores\n",
    "class RerankDiffMaskData():\n",
    "    def __init__(self, data_path=\"../data/T5_results/msmarco_test_t5.jsonl\", seed=42, reason_name = \"reason\"):\n",
    "        super()\n",
    "        self.data_path = data_path\n",
    "        self.data  = [json.loads(line) for line in open(data_path)]\n",
    "        self.data = receive_response(self.data, reason_name)\n",
    "        self.data = self.prepare_data()\n",
    "        self.seed = seed\n",
    "\n",
    "    def prepare_data(self):\n",
    "        query = []\n",
    "        docs = []\n",
    "        docs_reasons = []\n",
    "        unsorted_score = []\n",
    "        for item in self.data:\n",
    "            # print(item[\"query\"])\n",
    "            query.append(item[\"query\"])\n",
    "            docs.append(item[\"retrieved_passages\"])\n",
    "            docs_reasons.append(i + j for i, j in zip(item[\"retrieved_passages\"], item[\"reasoned_passages\"]))\n",
    "            unsorted_score.append(item[\"unsorted_score\"])\n",
    "        print(\"type(docs_reasons) in prepare data\", type(docs_reasons))\n",
    "        data = list(zip(query, docs, docs_reasons, unsorted_score))\n",
    "        query, docs, docs_reasons, unsorted_score = data[0]\n",
    "\n",
    "        print(\"type(docs_reasons) in prepare data after\", type(docs_reasons))\n",
    "        return data\n",
    "    \n",
    "    def get_dataloaders(self, batch_size, shuffle=True, val_split=0.1):\n",
    "        train_data, test_data = train_test_split(self.data, test_size=val_split, random_state=self.seed)\n",
    "        print(\"train_data\", type(train_data))\n",
    "        train_dataset = Dataset(train_data)\n",
    "        val_dataset = Dataset(test_data)\n",
    "        return (\n",
    "        DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle), \n",
    "        DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "data = RerankDiffMaskData(seed=1)\n",
    "train_dataloader, val_dataloader = data.get_dataloaders(batch_size=1, shuffle=True, val_split=0.00001)  \n",
    "\n",
    "print(len(train_dataloader.dataset.data))\n",
    "print(len(val_dataloader.dataset.data))\n",
    "for batch in train_dataloader:\n",
    "   query, docs, docs_reasons, unsorted_score = batch\n",
    "   # print(query)\n",
    "   # print(docs)\n",
    "   # print(X[0]+ \" she\")\n",
    "   # print(y[0].item())\n",
    "   break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f7e0fd-05a8-4ad4-aaaa-3db70f60b469",
   "metadata": {},
   "source": [
    "## Injection of Attention\n",
    "This is for changing the deberta model's injected attention and visualized the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a499bec6-03b9-420c-8f76-415e88228449",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffmask import DiffMask\n",
    "\n",
    "\n",
    "\n",
    "def attention_intervention_hook(\n",
    "    value: Float[torch.Tensor, \"batch pos head_index d_head\"],\n",
    "    hook: HookPoint,\n",
    "    counterfactual_cache: ActivationCache,\n",
    "    mask: torch.Tensor,\n",
    "    tail_indices: torch.Tensor,\n",
    "    cf_tail_indices: torch.Tensor,\n",
    ") -> Float[torch.Tensor, \"batch pos head_index d_head\"]:\n",
    "    b, p, h, d = value.shape\n",
    "    tail_indices = tail_indices.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).repeat(1, 1, h, d) \n",
    "    cf_tail_indices = cf_tail_indices.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).repeat(1, 1, h, d)\n",
    "    counterfactual_value = counterfactual_cache[hook.name]\n",
    "\n",
    "    v_select = torch.gather(value, 1, tail_indices)\n",
    "    cf_select = torch.gather(counterfactual_value, 1, cf_tail_indices)\n",
    "    mask = mask.unsqueeze(1).unsqueeze(-1).repeat(1, 1, 1, d)\n",
    "\n",
    "    intervention = (1-mask) * v_select + mask * cf_select\n",
    "    return torch.scatter(value, dim=1, index=tail_indices, src=intervention)\n",
    "\n",
    "class RankDiffMask(DiffMask):\n",
    "    def __init__(self, config: DiffMaskConfig, device):\n",
    "        super().__init__(config=config)\n",
    "        self.config = config\n",
    "        self.automatic_optimization = False\n",
    "        self.model = HookedTransformer.from_pretrained(config.mask.model, device=device)\n",
    "        self.model.cfg.use_attn_result = True\n",
    "        self.location = torch.nn.Parameter(torch.zeros((self.model.cfg.n_layers, self.model.cfg.n_heads)), requires_grad=True)\n",
    "        self.device = device\n",
    "\n",
    "    def intervene(self, inputs, mask):\n",
    "            \"\"\"\n",
    "            Args:\n",
    "                inputs: The original inputs (query, docs, reasons).\n",
    "                mask: The mask to apply.\n",
    "            Returns:\n",
    "                The logits of the original and intervened sequences.\n",
    "            \"\"\"\n",
    "            query, docs, reasons = inputs\n",
    "            reasons_masked = reasons * mask\n",
    "    \n",
    "            original_output = self.model(query, docs, reasons).logits\n",
    "            intervened_output = self.model(query, docs, reasons_masked).logits\n",
    "            return original_output, intervened_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8036eb5e-8a05-4283-9d3d-7b1614a554db",
   "metadata": {},
   "source": [
    "### Define the rank net loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32657cfd-b3b6-438a-a053-e77b6c8a13f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_net(y_pred, y_true=None, padded_value_indicator=-100, weight_by_diff=False,\n",
    "                 weight_by_diff_powed=False):\n",
    "        \"\"\"\n",
    "        RankNet loss introduced in \"Learning to Rank using Gradient Descent\".\n",
    "        :param y_pred: predictions from the model, shape [batch_size, slate_length]\n",
    "        :param y_true: ground truth labels, shape [batch_size, slate_length]\n",
    "        :param weight_by_diff: flag indicating whether to weight the score differences by ground truth differences.\n",
    "        :param weight_by_diff_powed: flag indicating whether to weight the score differences by the squared ground truth differences.\n",
    "        :return: loss value, a torch.Tensor\n",
    "        \"\"\"\n",
    "        if y_true is None:\n",
    "            y_true = torch.zeros_like(y_pred).to(y_pred.device)\n",
    "            y_true[:, 0] = 1\n",
    "\n",
    "        # here we generate every pair of indices from the range of document length in the batch\n",
    "        document_pairs_candidates = list(product(range(y_true.shape[1]), repeat=2))\n",
    "\n",
    "        pairs_true = y_true[:, document_pairs_candidates]\n",
    "        selected_pred = y_pred[:, document_pairs_candidates]\n",
    "\n",
    "        # here we calculate the relative true relevance of every candidate pair\n",
    "        true_diffs = pairs_true[:, :, 0] - pairs_true[:, :, 1]\n",
    "        pred_diffs = selected_pred[:, :, 0] - selected_pred[:, :, 1]\n",
    "\n",
    "        # here we filter just the pairs that are 'positive' and did not involve a padded instance\n",
    "        # we can do that since in the candidate pairs we had symetric pairs so we can stick with\n",
    "        # positive ones for a simpler loss function formulation\n",
    "        the_mask = (true_diffs > 0) & (~torch.isinf(true_diffs))\n",
    "\n",
    "        pred_diffs = pred_diffs[the_mask]\n",
    "\n",
    "        weight = None\n",
    "        if weight_by_diff:\n",
    "            abs_diff = torch.abs(true_diffs)\n",
    "            weight = abs_diff[the_mask]\n",
    "        elif weight_by_diff_powed:\n",
    "            true_pow_diffs = torch.pow(pairs_true[:, :, 0], 2) - torch.pow(pairs_true[:, :, 1], 2)\n",
    "            abs_diff = torch.abs(true_pow_diffs)\n",
    "            weight = abs_diff[the_mask]\n",
    "\n",
    "        # here we 'binarize' true relevancy diffs since for a pairwise loss we just need to know\n",
    "        # whether one document is better than the other and not about the actual difference in\n",
    "        # their relevancy levels\n",
    "        true_diffs = (true_diffs > 0).type(torch.float32)\n",
    "        true_diffs = true_diffs[the_mask]\n",
    "\n",
    "        return BCEWithLogitsLoss(weight=weight)(pred_diffs, true_diffs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f224c1-7e6e-4378-945a-4256a2573a66",
   "metadata": {},
   "source": [
    "### Example of the diffmask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e137cf8b-e2af-4aec-862d-bab78d1d056e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionDiffMask(DiffMask):\n",
    "    def __init__(self, config: DiffMaskConfig, model, device):\n",
    "        super().__init__(config=config)\n",
    "        self.config = config\n",
    "        self.automatic_optimization = False\n",
    "        self.model = model.to(device)\n",
    "        self.location = torch.nn.Parameter(torch.zeros((self.model.config.num_hidden_layers, self.model.config.num_attention_heads)), requires_grad=True)\n",
    "\n",
    "    def intervene(self, inputs, mask, ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            inputs: The original inputs (query, docs, reasons).\n",
    "            mask: The mask to apply.\n",
    "        Returns:\n",
    "            The logits of the original and intervened sequences.\n",
    "        \"\"\"\n",
    "        query, docs, reasons = inputs # inputs can be split tokens\n",
    "        with_reason = []\n",
    "        without_reason = []\n",
    "        for doc, reason in zip(docs, reasons):\n",
    "            with_reason.append()\n",
    "        \n",
    "        query_token =  self.model.to_tokens(query, prepend_bos=False)\n",
    "        \n",
    "        reasons_masked = reasons * mask\n",
    "\n",
    "        original_output = self.model(query, docs, reasons).logits\n",
    "        intervened_output = self.model(query, docs, reasons_masked).logits\n",
    "        return original_output, intervened_output\n",
    "\n",
    "    def calculate_ranknet_loss(self, scores, labels):\n",
    "        \"\"\"\n",
    "        Calculates the RankNet loss.\n",
    "        Args:\n",
    "            scores: The predicted scores for each query-document pair.\n",
    "            labels: The true relevance labels for each query-document pair.\n",
    "        Returns:\n",
    "            The RankNet loss.\n",
    "        \"\"\"\n",
    "        loss = 0.0\n",
    "        batch_size = scores.size(0)\n",
    "        for i in range(batch_size):\n",
    "            for j in range(batch_size):\n",
    "                if labels[i] < labels[j]:\n",
    "                    loss += torch.log(1 + torch.exp(scores[i] - scores[j]))\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx=None, optimizer_idx=None):\n",
    "        query, docs, reasons = batch\n",
    "        dist = RectifiedStreched(\n",
    "            BinaryConcrete(torch.full_like(self.location, 0.2), self.location), l=-0.2, r=1.0,\n",
    "        )\n",
    "        mask = dist.rsample(torch.Size([len(query), reasons.size(1), reasons.size(2)]))\n",
    "        expected_L0 = dist.expected_L0().sum()\n",
    "\n",
    "        original_output, intervened_output = self.intervene((query, docs, reasons), mask=mask)\n",
    "\n",
    "        o_ranknet_loss = self.calculate_ranknet_loss(original_output)\n",
    "        i_ranknet_loss = self.calculate_ranknet_loss(intervened_output)\n",
    "        out = model(**batch)\n",
    "        logits = out.logits\n",
    "        logits = logits.view(-1, neg_num)\n",
    "\n",
    "        y_true = torch.tensor([[1 / (i + 1) for i in range(logits.size(1))]] * logits.size(0)).cuda()\n",
    "        loss = loss_function(logits, y_true)\n",
    "\n",
    "        \n",
    "        kl_loss = torch.distributions.kl_divergence(\n",
    "            torch.distributions.Bernoulli(logits=original_output),\n",
    "            torch.distributions.Bernoulli(logits=intervened_output),\n",
    "        ).mean()\n",
    "\n",
    "        total_loss = ranknet_loss + kl_loss + self.lambda1 * (expected_L0 - self.config.mask.attn_heads)\n",
    "\n",
    "        self.manual_backward(total_loss)\n",
    "        o1, o2 = self.optimizers()\n",
    "\n",
    "        self.optimizer_step(o1, 0)\n",
    "        self.optimizer_step(o2, 1)\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        print(f\"lambda1: {self.lambda1}\")\n",
    "        print(f\"location: {self.location}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e2f724-7d75-4ade-9655-59efa60ac3d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad375ae-edee-4c83-8c76-2f262fc6598f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
