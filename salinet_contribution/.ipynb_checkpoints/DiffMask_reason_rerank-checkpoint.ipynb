{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8e54de3-7840-4a2f-92dd-081a5eaa0c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuj49/anaconda3/envs/llama_factory/lib/python3.8/site-packages/requests/__init__.py:87: RequestsDependencyWarning: urllib3 (2.2.1) or chardet (4.0.0) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import lightning.pytorch as pl\n",
    "from functools import partial\n",
    "from jaxtyping import Float\n",
    "from diffmask import DiffMask\n",
    "from util.distributions import BinaryConcrete, RectifiedStreched\n",
    "from configuration.diffmask import DiffMaskConfig\n",
    "import json\n",
    "from torch.utils.data import Dataset\n",
    "from accelerate import Accelerator\n",
    "from transformers import AutoModelForSequenceClassification, AutoConfig, AutoTokenizer, AdamW\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from rank_loss import RankLoss\n",
    "import numpy as np\n",
    "import os\n",
    "import argparse\n",
    "import tempfile\n",
    "import copy\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81e1dd3-3664-49f4-b449-d6e7762429ea",
   "metadata": {},
   "source": [
    "## Data process "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1a99e3f-0d98-4821-8651-cea2c14cf3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sperate_reason(reason_text):\n",
    "  pattern = r'\\-\\s(?:(?:Passage\\s)?\\[(\\d+)\\](?:\\s?and\\s?(?:Passage\\s)?\\[(\\d+)\\])?)(.*?)(?=\\n\\-|\\n\\n|\\Z)'\n",
    "\n",
    "  # print(\"reason_text in sperate_reason\", reason_text)\n",
    "  matches = re.findall(pattern, reason_text, re.DOTALL)\n",
    "\n",
    "  # 初始化一个字典来存储每个文档的reason\n",
    "  reasons_dict = {}\n",
    "  if len(matches) < 5:\n",
    "    # 处理匹配结果\n",
    "    for match in matches:\n",
    "        # 提取文档编号和reason描述\n",
    "        doc_ids = match[:-1]  # 文档编号部分\n",
    "        doc_reason = match[-1].strip()  # reason描述部分\n",
    "\n",
    "        # 对每个文档编号进行处理\n",
    "        for doc_id in doc_ids:\n",
    "            if doc_id:  # 确保doc_id不为空\n",
    "                # 为每个文档编号存储或更新reason描述\n",
    "                if doc_id in reasons_dict:\n",
    "                    # 如果同一个文档编号对应多个reason，可以选择合并或选择性保留\n",
    "                    reasons_dict[doc_id] += \" \" + doc_reason\n",
    "                else:\n",
    "                    reasons_dict[doc_id] = doc_reason\n",
    "  else:\n",
    "    for match in matches:\n",
    "      # print(\"match\", match)\n",
    "      doc_id, _, doc_reason = match\n",
    "      reasons_dict[doc_id] = doc_reason\n",
    "          # print(f\"Document ID: {doc_id}, Reason: {doc_reason}\\n\")\n",
    "\n",
    "  if len(reasons_dict.keys()) <5:\n",
    "    \n",
    "    return None\n",
    "  \n",
    "\n",
    "  return reasons_dict\n",
    "\n",
    "\n",
    "def receive_response(data, reason_name=\"reason\"):\n",
    "    \n",
    "    responses = [item[\"re_rank_id\"] for item in data]\n",
    "    \n",
    "    def remove_duplicate(response):\n",
    "        new_response = []\n",
    "        for c in response:\n",
    "            if c not in new_response:\n",
    "                new_response.append(c)\n",
    "        return new_response\n",
    "\n",
    "    new_data = []\n",
    "    unsorted_score = []\n",
    "    for item, response in zip(data, responses):\n",
    "        \n",
    "        reasons = item[reason_name]\n",
    "        reasons_dict = sperate_reason(reasons)\n",
    "        \n",
    "        passages = item['unsorted_docs']\n",
    "        \n",
    "        unsorted_reasoned_response = [] \n",
    "        \n",
    "        if reasons_dict!=None:\n",
    "            for idx, passage in enumerate(passages):\n",
    "                unsorted_reasoned_response.append(passage + \"reason\" +reasons_dict[str(idx+1)] )\n",
    "        else:\n",
    "            unsorted_reasoned_response = [\"\"]*5\n",
    "        \n",
    "        \n",
    "        response = [int(x) - 1 for x in response]\n",
    "        response = remove_duplicate(response)\n",
    "        \n",
    "        original_rank = [tt for tt in range(len(passages))]\n",
    "        response = [ss for ss in response if ss in original_rank]\n",
    "        response = response + [tt for tt in original_rank if tt not in response]\n",
    "        \n",
    "        new_passages = [passages[ii] for ii in response]\n",
    "        new_reason_passage = [unsorted_reasoned_response[ii] for ii in response]\n",
    "        unsorted_score = item[\"scores\"]\n",
    "\n",
    "        \n",
    "        new_data.append({'query': item['query'],\n",
    "                         'retrieved_passages': new_passages,\n",
    "                         'reasoned_passages':new_reason_passage,\n",
    "                        'unsorted_score':unsorted_score})\n",
    "    return new_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a757062-6ca4-4f86-a959-d4164291951c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_path =\"../data/T5_results/msmarco_test_t5.jsonl\"\n",
    "data = [json.loads(line) for line in open(data_path)]\n",
    "data = receive_response(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dee42442-f83f-49a9-9b6b-592fa8bf62e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data [{\"query\":'', \"retrieved_passages\":[docs], \"reasoned_passages\":[],\"unsorted_score\":[]}]\n",
    "# print(len(data[0][\"reasoned_passages\"])) # 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a085d8c2-a870-4bcd-b624-4f99a937a0ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(docs_reasons) in prepare data <class 'list'>\n",
      "type(docs_reasons) in prepare data after <class 'generator'>\n",
      "train_data <class 'list'>\n",
      "171\n",
      "1\n",
      "type(docs_reasons) <class 'generator'>\n",
      "type(unsorted_scores) <class 'list'>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'generator'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 65\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(train_dataloader\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mdata))\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(val_dataloader\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mdata))\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m train_dataloader:\n\u001b[1;32m     66\u001b[0m    query, docs, docs_reasons, unsorted_score \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m     67\u001b[0m    \u001b[38;5;66;03m# print(query)\u001b[39;00m\n\u001b[1;32m     68\u001b[0m    \u001b[38;5;66;03m# print(docs)\u001b[39;00m\n\u001b[1;32m     69\u001b[0m    \u001b[38;5;66;03m# print(X[0]+ \" she\")\u001b[39;00m\n\u001b[1;32m     70\u001b[0m    \u001b[38;5;66;03m# print(y[0].item())\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/llama_factory/lib/python3.8/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/llama_factory/lib/python3.8/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/llama_factory/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/llama_factory/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py:316\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    256\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/llama_factory/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py:173\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    170\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m--> 173\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/llama_factory/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py:173\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    170\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m--> 173\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/llama_factory/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py:191\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m             \u001b[38;5;66;03m# The sequence type may not support `copy()` / `__setitem__(index, item)`\u001b[39;00m\n\u001b[1;32m    188\u001b[0m             \u001b[38;5;66;03m# or `__init__(iterable)` (e.g., `range`).\u001b[39;00m\n\u001b[1;32m    189\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]\n\u001b[0;32m--> 191\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(default_collate_err_msg_format\u001b[38;5;241m.\u001b[39mformat(elem_type))\n",
      "\u001b[0;31mTypeError\u001b[0m: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'generator'>"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from os import path, makedirs\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class Dataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        query, docs, docs_reasons, unsorted_scores= self.data[index]\n",
    "        print(\"type(docs_reasons)\", type(docs_reasons))\n",
    "        print(\"type(unsorted_scores)\", type(unsorted_scores))\n",
    "        # return query, docs, docs_reasons, unsorted_scores\n",
    "        return query, docs, list(docs_reasons), unsorted_scores\n",
    "class RerankDiffMaskData():\n",
    "    def __init__(self, data_path=\"../data/T5_results/msmarco_test_t5.jsonl\", seed=42, reason_name = \"reason\"):\n",
    "        super()\n",
    "        self.data_path = data_path\n",
    "        self.data  = [json.loads(line) for line in open(data_path)]\n",
    "        self.data = receive_response(self.data, reason_name)\n",
    "        self.data = self.prepare_data()\n",
    "        self.seed = seed\n",
    "\n",
    "    def prepare_data(self):\n",
    "        query = []\n",
    "        docs = []\n",
    "        docs_reasons = []\n",
    "        unsorted_score = []\n",
    "        for item in self.data:\n",
    "            # print(item[\"query\"])\n",
    "            query.append(item[\"query\"])\n",
    "            docs.append(item[\"retrieved_passages\"])\n",
    "            docs_reasons.append(i + j for i, j in zip(item[\"retrieved_passages\"], item[\"reasoned_passages\"]))\n",
    "            unsorted_score.append(item[\"unsorted_score\"])\n",
    "        print(\"type(docs_reasons) in prepare data\", type(docs_reasons))\n",
    "        data = list(zip(query, docs, docs_reasons, unsorted_score))\n",
    "        query, docs, docs_reasons, unsorted_score = data[0]\n",
    "\n",
    "        print(\"type(docs_reasons) in prepare data after\", type(docs_reasons))\n",
    "        return data\n",
    "    \n",
    "    def get_dataloaders(self, batch_size, shuffle=True, val_split=0.1):\n",
    "        train_data, test_data = train_test_split(self.data, test_size=val_split, random_state=self.seed)\n",
    "        print(\"train_data\", type(train_data))\n",
    "        train_dataset = Dataset(train_data)\n",
    "        val_dataset = Dataset(test_data)\n",
    "        return (\n",
    "        DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle), \n",
    "        DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "data = RerankDiffMaskData(seed=1)\n",
    "train_dataloader, val_dataloader = data.get_dataloaders(batch_size=1, shuffle=True, val_split=0.00001)  \n",
    "\n",
    "print(len(train_dataloader.dataset.data))\n",
    "print(len(val_dataloader.dataset.data))\n",
    "for batch in train_dataloader:\n",
    "   query, docs, docs_reasons, unsorted_score = batch\n",
    "   # print(query)\n",
    "   # print(docs)\n",
    "   # print(X[0]+ \" she\")\n",
    "   # print(y[0].item())\n",
    "   break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f7e0fd-05a8-4ad4-aaaa-3db70f60b469",
   "metadata": {},
   "source": [
    "## Injection of Attention\n",
    "This is for changing the deberta model's injected attention and visualized the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a499bec6-03b9-420c-8f76-415e88228449",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffmask import DiffMask\n",
    "\n",
    "\n",
    "\n",
    "def attention_intervention_hook(\n",
    "    value: Float[torch.Tensor, \"batch pos head_index d_head\"],\n",
    "    hook: HookPoint,\n",
    "    counterfactual_cache: ActivationCache,\n",
    "    mask: torch.Tensor,\n",
    "    tail_indices: torch.Tensor,\n",
    "    cf_tail_indices: torch.Tensor,\n",
    ") -> Float[torch.Tensor, \"batch pos head_index d_head\"]:\n",
    "    b, p, h, d = value.shape\n",
    "    tail_indices = tail_indices.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).repeat(1, 1, h, d) \n",
    "    cf_tail_indices = cf_tail_indices.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).repeat(1, 1, h, d)\n",
    "    counterfactual_value = counterfactual_cache[hook.name]\n",
    "\n",
    "    v_select = torch.gather(value, 1, tail_indices)\n",
    "    cf_select = torch.gather(counterfactual_value, 1, cf_tail_indices)\n",
    "    mask = mask.unsqueeze(1).unsqueeze(-1).repeat(1, 1, 1, d)\n",
    "\n",
    "    intervention = (1-mask) * v_select + mask * cf_select\n",
    "    return torch.scatter(value, dim=1, index=tail_indices, src=intervention)\n",
    "\n",
    "class RankDiffMask(DiffMask):\n",
    "    def __init__(self, config: DiffMaskConfig, device):\n",
    "        super().__init__(config=config)\n",
    "        self.config = config\n",
    "        self.automatic_optimization = False\n",
    "        self.model = HookedTransformer.from_pretrained(config.mask.model, device=device)\n",
    "        self.model.cfg.use_attn_result = True\n",
    "        self.location = torch.nn.Parameter(torch.zeros((self.model.cfg.n_layers, self.model.cfg.n_heads)), requires_grad=True)\n",
    "        self.device = device\n",
    "\n",
    "    def intervene(self, inputs, mask):\n",
    "            \"\"\"\n",
    "            Args:\n",
    "                inputs: The original inputs (query, docs, reasons).\n",
    "                mask: The mask to apply.\n",
    "            Returns:\n",
    "                The logits of the original and intervened sequences.\n",
    "            \"\"\"\n",
    "            query, docs, reasons = inputs\n",
    "            reasons_masked = reasons * mask\n",
    "    \n",
    "            original_output = self.model(query, docs, reasons).logits\n",
    "            intervened_output = self.model(query, docs, reasons_masked).logits\n",
    "            return original_output, intervened_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8036eb5e-8a05-4283-9d3d-7b1614a554db",
   "metadata": {},
   "source": [
    "### Define the rank net loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32657cfd-b3b6-438a-a053-e77b6c8a13f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_net(y_pred, y_true=None, padded_value_indicator=-100, weight_by_diff=False,\n",
    "                 weight_by_diff_powed=False):\n",
    "        \"\"\"\n",
    "        RankNet loss introduced in \"Learning to Rank using Gradient Descent\".\n",
    "        :param y_pred: predictions from the model, shape [batch_size, slate_length]\n",
    "        :param y_true: ground truth labels, shape [batch_size, slate_length]\n",
    "        :param weight_by_diff: flag indicating whether to weight the score differences by ground truth differences.\n",
    "        :param weight_by_diff_powed: flag indicating whether to weight the score differences by the squared ground truth differences.\n",
    "        :return: loss value, a torch.Tensor\n",
    "        \"\"\"\n",
    "        if y_true is None:\n",
    "            y_true = torch.zeros_like(y_pred).to(y_pred.device)\n",
    "            y_true[:, 0] = 1\n",
    "\n",
    "        # here we generate every pair of indices from the range of document length in the batch\n",
    "        document_pairs_candidates = list(product(range(y_true.shape[1]), repeat=2))\n",
    "\n",
    "        pairs_true = y_true[:, document_pairs_candidates]\n",
    "        selected_pred = y_pred[:, document_pairs_candidates]\n",
    "\n",
    "        # here we calculate the relative true relevance of every candidate pair\n",
    "        true_diffs = pairs_true[:, :, 0] - pairs_true[:, :, 1]\n",
    "        pred_diffs = selected_pred[:, :, 0] - selected_pred[:, :, 1]\n",
    "\n",
    "        # here we filter just the pairs that are 'positive' and did not involve a padded instance\n",
    "        # we can do that since in the candidate pairs we had symetric pairs so we can stick with\n",
    "        # positive ones for a simpler loss function formulation\n",
    "        the_mask = (true_diffs > 0) & (~torch.isinf(true_diffs))\n",
    "\n",
    "        pred_diffs = pred_diffs[the_mask]\n",
    "\n",
    "        weight = None\n",
    "        if weight_by_diff:\n",
    "            abs_diff = torch.abs(true_diffs)\n",
    "            weight = abs_diff[the_mask]\n",
    "        elif weight_by_diff_powed:\n",
    "            true_pow_diffs = torch.pow(pairs_true[:, :, 0], 2) - torch.pow(pairs_true[:, :, 1], 2)\n",
    "            abs_diff = torch.abs(true_pow_diffs)\n",
    "            weight = abs_diff[the_mask]\n",
    "\n",
    "        # here we 'binarize' true relevancy diffs since for a pairwise loss we just need to know\n",
    "        # whether one document is better than the other and not about the actual difference in\n",
    "        # their relevancy levels\n",
    "        true_diffs = (true_diffs > 0).type(torch.float32)\n",
    "        true_diffs = true_diffs[the_mask]\n",
    "\n",
    "        return BCEWithLogitsLoss(weight=weight)(pred_diffs, true_diffs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f224c1-7e6e-4378-945a-4256a2573a66",
   "metadata": {},
   "source": [
    "### Example of the diffmask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e137cf8b-e2af-4aec-862d-bab78d1d056e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionDiffMask(DiffMask):\n",
    "    def __init__(self, config: DiffMaskConfig, model, device):\n",
    "        super().__init__(config=config)\n",
    "        self.config = config\n",
    "        self.automatic_optimization = False\n",
    "        self.model = model.to(device)\n",
    "        self.location = torch.nn.Parameter(torch.zeros((self.model.config.num_hidden_layers, self.model.config.num_attention_heads)), requires_grad=True)\n",
    "\n",
    "    def intervene(self, inputs, mask, ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            inputs: The original inputs (query, docs, reasons).\n",
    "            mask: The mask to apply.\n",
    "        Returns:\n",
    "            The logits of the original and intervened sequences.\n",
    "        \"\"\"\n",
    "        query, docs, reasons = inputs # inputs can be split tokens\n",
    "        with_reason = []\n",
    "        without_reason = []\n",
    "        for doc, reason in zip(docs, reasons):\n",
    "            with_reason.append()\n",
    "        \n",
    "        query_token =  self.model.to_tokens(query, prepend_bos=False)\n",
    "        \n",
    "        reasons_masked = reasons * mask\n",
    "\n",
    "        original_output = self.model(query, docs, reasons).logits\n",
    "        intervened_output = self.model(query, docs, reasons_masked).logits\n",
    "        return original_output, intervened_output\n",
    "\n",
    "    def calculate_ranknet_loss(self, scores, labels):\n",
    "        \"\"\"\n",
    "        Calculates the RankNet loss.\n",
    "        Args:\n",
    "            scores: The predicted scores for each query-document pair.\n",
    "            labels: The true relevance labels for each query-document pair.\n",
    "        Returns:\n",
    "            The RankNet loss.\n",
    "        \"\"\"\n",
    "        loss = 0.0\n",
    "        batch_size = scores.size(0)\n",
    "        for i in range(batch_size):\n",
    "            for j in range(batch_size):\n",
    "                if labels[i] < labels[j]:\n",
    "                    loss += torch.log(1 + torch.exp(scores[i] - scores[j]))\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx=None, optimizer_idx=None):\n",
    "        query, docs, reasons = batch\n",
    "        dist = RectifiedStreched(\n",
    "            BinaryConcrete(torch.full_like(self.location, 0.2), self.location), l=-0.2, r=1.0,\n",
    "        )\n",
    "        mask = dist.rsample(torch.Size([len(query), reasons.size(1), reasons.size(2)]))\n",
    "        expected_L0 = dist.expected_L0().sum()\n",
    "\n",
    "        original_output, intervened_output = self.intervene((query, docs, reasons), mask=mask)\n",
    "\n",
    "        o_ranknet_loss = self.calculate_ranknet_loss(original_output)\n",
    "        i_ranknet_loss = self.calculate_ranknet_loss(intervened_output)\n",
    "        out = model(**batch)\n",
    "        logits = out.logits\n",
    "        logits = logits.view(-1, neg_num)\n",
    "\n",
    "        y_true = torch.tensor([[1 / (i + 1) for i in range(logits.size(1))]] * logits.size(0)).cuda()\n",
    "        loss = loss_function(logits, y_true)\n",
    "\n",
    "        \n",
    "        kl_loss = torch.distributions.kl_divergence(\n",
    "            torch.distributions.Bernoulli(logits=original_output),\n",
    "            torch.distributions.Bernoulli(logits=intervened_output),\n",
    "        ).mean()\n",
    "\n",
    "        total_loss = ranknet_loss + kl_loss + self.lambda1 * (expected_L0 - self.config.mask.attn_heads)\n",
    "\n",
    "        self.manual_backward(total_loss)\n",
    "        o1, o2 = self.optimizers()\n",
    "\n",
    "        self.optimizer_step(o1, 0)\n",
    "        self.optimizer_step(o2, 1)\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        print(f\"lambda1: {self.lambda1}\")\n",
    "        print(f\"location: {self.location}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e2f724-7d75-4ade-9655-59efa60ac3d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad375ae-edee-4c83-8c76-2f262fc6598f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
